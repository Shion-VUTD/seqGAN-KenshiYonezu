{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9E1ZjQ_-23Q",
        "colab_type": "text"
      },
      "source": [
        "データのスクレイピング（URLの部分に、pdfに記載のURLを入力して実行）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxkAVBuW-553",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import bs4\n",
        "import time\n",
        "import requests\n",
        "import pprint\n",
        "\n",
        "def load(url):\n",
        "    res = requests.get(url)\n",
        "    res.raise_for_status()\n",
        "\n",
        "    return res.text\n",
        "\n",
        "def pickup_tag(html, find_tag):\n",
        "    soup = bs4.BeautifulSoup(str(html), 'html.parser')\n",
        "    paragraphs = soup.find_all(find_tag)\n",
        "\n",
        "    return paragraphs\n",
        "\n",
        "def parse(html):\n",
        "    soup = bs4.BeautifulSoup(str(html), 'html.parser')\n",
        "    # htmlタグの排除\n",
        "    kashi_row = soup.getText()\n",
        "    kashi_row = kashi_row.replace('\\n', '')\n",
        "    kashi_row = kashi_row.replace('　', '')\n",
        "\n",
        "    # 英数字の排除\n",
        "    kashi_row = re.sub(r'[a-zA-Z0-9]', '', kashi_row)\n",
        "    # 記号の排除\n",
        "    kashi_row = re.sub(r'[ ＜＞♪`‘’“”・…_！？!-/:-@[-`{-~]', '', kashi_row)\n",
        "    # 注意書きの排除\n",
        "    kashi = re.sub(r'注意：.+', '', kashi_row)\n",
        "\n",
        "    return kashi\n",
        "\n",
        "def main():\n",
        "    with open('yonedu_kashi.txt', 'a') as f:\n",
        "        # アーティストページのアドレス\n",
        "        url = f'https://www.uta-net.com/search/?Aselect=6&Bselect=3&Keyword=あ&sort=&pnum=1'\n",
        "        \n",
        "        # 曲ページの先頭アドレス\n",
        "        base_url = f'https://www.uta-net.com'\n",
        "\n",
        "        # ページの取得\n",
        "        html = load(url)\n",
        "\n",
        "        # 曲ごとのurlを格納\n",
        "        musics_url = []\n",
        "        # 歌詞を格納\n",
        "        kashis = ''\n",
        "\n",
        "        \"\"\" 曲のurlを取得 \"\"\"\n",
        "        # td要素の取り出し\n",
        "        for td in pickup_tag(html, 'td'):\n",
        "            # a要素の取り出し\n",
        "            for a in pickup_tag(td, 'a'):\n",
        "                # href属性にsongを含むか\n",
        "                if 'song' in a.get('href'): \n",
        "                    # urlを配列に追加\n",
        "                    musics_url.append(base_url + a.get('href'))\n",
        "        # pprint.pprint(musics_url)\n",
        "\n",
        "        \"\"\" 歌詞の取得 \"\"\"\n",
        "        for i, page in enumerate(musics_url):\n",
        "            print('{}曲目:{}'.format(i + 1, page))\n",
        "            html = load(page)\n",
        "            for div in pickup_tag(html, 'div'):\n",
        "                # id検索がうまく行えなかった為、一度strにキャスト\n",
        "                div = str(div)\n",
        "                # 歌詞が格納されているdiv要素か\n",
        "                if r'itemprop=\"text\"' in div:\n",
        "                    # 不要なデータを取り除く\n",
        "                    kashi = parse(div)\n",
        "\n",
        "                    print(kashi, end = '\\n\\n')\n",
        "                    # 歌詞を１つにまとめる\n",
        "                    kashis += kashi + '\\n'\n",
        "\n",
        "                    # １秒待機\n",
        "                    time.sleep(1)\n",
        "                    break\n",
        "        # 歌詞の書き込み\n",
        "        f.write(kashis)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0cMXGKxXR43",
        "colab_type": "text"
      },
      "source": [
        "LSTMを実行してみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUO0zPF_XdBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils.data_utils import get_file\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "\n",
        "path = '/Users/yamashitashiori/Desktop/python3/yonedu_kashi.txt'\n",
        "text = open(path, \"r\").read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "char_indices = dict((c,i) for i,c in enumerate(chars))\n",
        "indices_char = dict((i,c) for i,c in enumerate(chars))\n",
        "\n",
        "maxlen = 40\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "\n",
        "# テキストのベクトル化\n",
        "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "\n",
        "# モデルを定義する\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "for iteration in range(1,120):\n",
        "    print()\n",
        "    print('-' *50)\n",
        "    print('繰り返し回数: ', iteration)\n",
        "    model.fit(X, y, batch_size=128, epochs=1)\n",
        "\n",
        "    start_index = random.randint(0, len(text)-maxlen-1)\n",
        "\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print()\n",
        "        print('-----diveristy', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Seedを生成しました: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(100):\n",
        "            x = np.zeros((1,maxlen,len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCfK_lCE-100",
        "colab_type": "text"
      },
      "source": [
        "データの前処理（分かち書き）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBAlUeK7_o6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import MeCab\n",
        "tagger = MeCab.Tagger('-Owakati')\n",
        "with open(\"/Users/yamashitashiori/Desktop/Python3/kashi.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.readlines()\n",
        "    for line in text:\n",
        "        \n",
        "        result = tagger.parse(line)\n",
        "        with open(\"Desktop/Python3/kashi_wakati.txt\", \"a\", encoding=\"utf-8\") as of:\n",
        "                of.write(result[0:])\n",
        "        \n",
        "print(\"Finish\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvRZhDkN_0iT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import MeCab\n",
        "tagger = MeCab.Tagger('-Owakati')\n",
        "with open(\"/Users/yamashitashiori/Desktop/Python3/yonedu_kashi.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.readlines()\n",
        "    for line in text:\n",
        "        \n",
        "        result = tagger.parse(line)\n",
        "        with open(\"Desktop/Python3/yonezu_wakati.txt\", \"a\", encoding=\"utf-8\") as of:\n",
        "                of.write(result[0:])\n",
        "        \n",
        "print(\"Finish\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR3_g2ayAMSx",
        "colab_type": "text"
      },
      "source": [
        "データの前処理（id変換）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b9pOIgZARpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class EncoderDecoder(object):\n",
        "    def __init__(self, sentences, bos=False, eos=False):\n",
        "        # word_to_idの辞書\n",
        "        self.w2i = {}\n",
        "        # id_to_wordの辞書\n",
        "        self.i2w = {}\n",
        "        # 文頭の記号を入れるか\n",
        "        self.bos = bos\n",
        "        # 文末の記号を入れるか\n",
        "        self.eos = eos\n",
        "        # 予約語(パディング, 文章の始まり)\n",
        "        self.special_chars = ['<pad>', '<s>', '</s>', '<unk>']\n",
        "        self.bos_char = self.special_chars[1]\n",
        "        self.eos_char = self.special_chars[2]\n",
        "        self.oov_char = self.special_chars[3]\n",
        "        # 全ての単語を読み込んで辞書作成\n",
        "        self.fit(sentences)\n",
        "\n",
        "    # コールされる関数\n",
        "    def __call__(self, sentence):\n",
        "        return self.transform(sentence)\n",
        "\n",
        "    # 辞書作成\n",
        "    def fit(self, sentences):\n",
        "        self._words = set()\n",
        "\n",
        "        # 未知の単語の集合を作成する\n",
        "        for sentence in sentences:\n",
        "            self._words.update(sentence)\n",
        "\n",
        "        # 予約語分ずらしてidを振る\n",
        "        self.w2i = {w: (i + len(self.special_chars))\n",
        "                    for i, w in enumerate(self._words)}\n",
        "\n",
        "        # 予約語を辞書に追加する(<pad>:0, <s>:1, </s>:2, <unk>:3)\n",
        "        for i, w in enumerate(self.special_chars):\n",
        "            self.w2i[w] = i\n",
        "\n",
        "        # word_to_idの辞書を用いてid_to_wordの辞書を作成する\n",
        "        self.i2w = {i: w for w, i in self.w2i.items()}\n",
        "\n",
        "    # 1文をidに変換する\n",
        "    def transform(self, sentence):\n",
        "        # 指定があれば始まりと終わりの記号を追加する\n",
        "        if self.bos:\n",
        "            sentence = [self.bos_char] + sentence\n",
        "        if self.eos:\n",
        "            sentence = sentence + [self.eos_char]\n",
        "        output = self.encode(sentence)\n",
        "\n",
        "        return output\n",
        "\n",
        "    # 1文ずつidにする\n",
        "    def encode(self, sentence):\n",
        "        output = []\n",
        "        for w in sentence:\n",
        "            if w not in self.w2i:\n",
        "                idx = self.w2i[self.oov_char]\n",
        "            else:\n",
        "                idx = self.w2i[w]\n",
        "            output.append(idx)\n",
        "\n",
        "        return output\n",
        "\n",
        "    # # １文ずつ単語リストに直す\n",
        "    # def decode(self, sentence):\n",
        "    #     return [self.i2w[id] for id in sentence]\n",
        "\n",
        "with open('/Users/yamashitashiori/Desktop/Python3/wakati.txt') as f:\n",
        "    l = f.readlines()\n",
        "    lst = []\n",
        "    for x in l:\n",
        "        x2 = x.replace('\\n','')\n",
        "        y = x2.split(' ')\n",
        "        lst.append(y)\n",
        "    \n",
        "    \n",
        "    transform = EncoderDecoder(lst, bos=True, eos=True)   \n",
        "    \n",
        "with open('Desktop/Python3/kashi_id.txt','w') as f:\n",
        "    for x in lst:\n",
        "        a = transform(x)\n",
        "        del a[0]\n",
        "        del a[-1]\n",
        "        \n",
        "        y = ' '.join([str(i) for i in a])\n",
        "        \n",
        "\n",
        "        f.write(y + '\\n')\n",
        "      \n",
        "      #ここから上80曲分を手動でyonezu_id.txtに移動"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBn9QLogA6vR",
        "colab_type": "text"
      },
      "source": [
        "本体（SeqGAN）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFBMjq-yA8z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.shoeisha.co.jp/static/book/download/9784798159928/RL_Book.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl1rVyQbBJTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip RL_Book.zip\n",
        "%cd ./RL_Book/\n",
        "!pip install -r docker/requirements.txt\n",
        "!git clone https://github.com/benelot/pybullet-gym\n",
        "%cd pybullet-gym\n",
        "!pip install -e .\n",
        "%cd ..\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "!pip install pyglet\n",
        "!pip install pyopengl\n",
        "!pip install pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEWrO8ujQxiO",
        "colab_type": "text"
      },
      "source": [
        "Tensorflowのバージョン変更"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn3UG51KBM3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWRyK0mVBRHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "def play_movie(mp4_path):\n",
        "  video = io.open(mp4_path, 'r+b').read()\n",
        "  encoded = base64.b64encode(video)\n",
        "  return HTML(data='''<video alt=\"test\" controls>\n",
        "                      <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                      </video>'''.format(encoded.decode('ascii')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Osa3wOF0BUQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd ./RL_Book/\n",
        "%ls contents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1wHuiWmRkIz",
        "colab_type": "text"
      },
      "source": [
        "コードはここから\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmS95Wy2BWrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class Agent(object):\n",
        "    def __init__(self, sess, vocab_size, emb_size, hidden_size,\n",
        "                 T, lr):\n",
        "        self.sess = sess\n",
        "        self.size = 1\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_size = emb_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lr = lr\n",
        "        self.T = T  # sentence_size\n",
        "\n",
        "        self.pre_generator = self._build_pre_generator(\n",
        "            vocab_size,\n",
        "            emb_size,\n",
        "            hidden_size\n",
        "        )\n",
        "        self.generator = Actor(\n",
        "            sess,\n",
        "            self.size,\n",
        "            vocab_size,\n",
        "            emb_size,\n",
        "            hidden_size,\n",
        "            T,\n",
        "            lr\n",
        "        )\n",
        "        self.rollouter = Actor(\n",
        "            sess,\n",
        "            self.size,\n",
        "            vocab_size,\n",
        "            emb_size,\n",
        "            hidden_size,\n",
        "            T,\n",
        "            lr\n",
        "        )\n",
        "\n",
        "    def _build_pre_generator(self, vocab_size, emb_size, hidden_size):\n",
        "        data_inp = Input(shape=(None, ), dtype='int32', name='input')\n",
        "        out = Embedding(\n",
        "            vocab_size, emb_size, mask_zero=False, name='embedding'\n",
        "        )(data_inp)\n",
        "        out = LSTM(hidden_size, return_sequences=True, name='LSTM')(out)\n",
        "        out = TimeDistributed(\n",
        "            Dense(vocab_size, activation='softmax', name='dense_softmax'),\n",
        "            name='time_dense_softmax')(out)\n",
        "        pre_generator = Model(data_inp, out)\n",
        "        return pre_generator\n",
        "\n",
        "    def pre_train(self, g_data, g_pre_episodes, weight_path, g_pre_lr):\n",
        "        g_optimizer = Adam(g_pre_lr)\n",
        "        self.pre_generator.compile(g_optimizer, 'categorical_crossentropy')\n",
        "        self.pre_generator.summary()\n",
        "        self.pre_hist = self.pre_generator.fit_generator(\n",
        "            g_data,\n",
        "            steps_per_epoch=None,\n",
        "            epochs=g_pre_episodes\n",
        "        )\n",
        "        self.pre_generator.save_weights(weight_path)\n",
        "        self.inherit_weights(self.pre_generator, self.generator)\n",
        "        self.inherit_weights(self.pre_generator, self.rollouter)\n",
        "\n",
        "    def sample_words(self, prob):\n",
        "        action = np.zeros((self.size, ), dtype=np.int32)\n",
        "        for i in range(self.size):\n",
        "            p = prob[i]\n",
        "            action[i] = np.random.choice(self.vocab_size, p=p)\n",
        "        return action\n",
        "\n",
        "    def sample_sentences(self, actor, T, BOS=1):\n",
        "        actor.reset_rnn_state()\n",
        "        action = np.zeros([self.size, 1], dtype=np.int32)\n",
        "        action[:, 0] = BOS\n",
        "        actions = action\n",
        "        for _ in range(T):\n",
        "            prob, _, _ = actor.predict(action)\n",
        "            action = self.sample_words(prob).reshape(-1, 1)\n",
        "            actions = np.concatenate([actions, action], axis=-1)\n",
        "        actions = actions[:, 1:]\n",
        "        actor.reset_rnn_state()\n",
        "        return actions\n",
        "\n",
        "    def generate_id_samples(self, actor, T, sample_num, output_file):\n",
        "        sentences_ids = []\n",
        "        for _ in range(sample_num):\n",
        "            actions = self.sample_sentences(actor, T)\n",
        "            actions_list = actions.tolist()\n",
        "            for ids in actions_list:\n",
        "                ids_str = [str(id) for id in ids]\n",
        "                sentences_ids.append(ids_str)\n",
        "        output_str = ''\n",
        "        for i in range(sample_num):\n",
        "            output_str += ' '.join(sentences_ids[i]) + '\\n'\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(output_str)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        s_t = state[:, -1:].reshape([-1, 1])\n",
        "        prob, h, c = self.generator.predict(s_t)\n",
        "        action_t = self.sample_words(prob).reshape([1, 1])\n",
        "        is_end = self.projection(state)\n",
        "        return action_t * is_end, h, c\n",
        "\n",
        "    def rollout_sampling(self, action, epsilon=0.0):\n",
        "        prob, h, c = self.rollouter.predict(action)\n",
        "        action_t = self.sample_words(prob).reshape([1, 1])\n",
        "        is_end = self.projection(action)\n",
        "        return action_t * is_end\n",
        "\n",
        "    def projection(self, state, PAD=0, EOS=2):\n",
        "        is_PAD = state[:, -1:] == PAD\n",
        "        is_EOS = state[:, -1:] == EOS\n",
        "        is_END = 1 - is_PAD.astype(np.int) - is_EOS.astype(np.int)\n",
        "        return is_END.reshape([1, 1])\n",
        "\n",
        "    def rollout(self, step, state, action):\n",
        "        Y_i = state[:, 1:]\n",
        "        Y_i = np.concatenate([Y_i, action], axis=-1)\n",
        "        for _ in range(self.T - 1 - step):\n",
        "            _action = self.rollout_sampling(action)\n",
        "            Y_i = np.concatenate([Y_i, _action], axis=-1)\n",
        "            action = _action\n",
        "        return Y_i\n",
        "\n",
        "    def inherit_weights(self, agent, to_agent):\n",
        "        i = 0\n",
        "        for layer in agent.layers:\n",
        "            if len(layer.get_weights()) != 0:\n",
        "                w = layer.get_weights()\n",
        "                to_agent.layers[i].set_weights(w)\n",
        "                i += 1\n",
        "\n",
        "    def initialize(self, g_pre_weight):\n",
        "        self.pre_generator.load_weights(g_pre_weight)\n",
        "        self.inherit_weights(self.pre_generator, self.generator)\n",
        "        self.inherit_weights(self.pre_generator, self.rollouter)\n",
        "\n",
        "    def reset_rnn_states(self):\n",
        "        self.generator.reset_rnn_state()\n",
        "        self.rollouter.reset_rnn_state()\n",
        "\n",
        "\n",
        "class Actor(object):\n",
        "    def __init__(self, sess, size, vocab_size, emb_size, hidden_size,\n",
        "                 T, lr):\n",
        "        self.sess = sess\n",
        "        self.size = size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_size = emb_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.T = T\n",
        "        self.lr = lr\n",
        "        self._build_graph()\n",
        "        self.reset_rnn_state()\n",
        "\n",
        "    def _build_graph(self):\n",
        "        state_in = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        h_in = tf.placeholder(tf.float32, shape=(None, self.hidden_size))\n",
        "        c_in = tf.placeholder(tf.float32, shape=(None, self.hidden_size))\n",
        "        action = tf.placeholder(tf.float32, shape=(None, self.vocab_size))\n",
        "        reward = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "\n",
        "        self.layers = []\n",
        "\n",
        "        embedding = Embedding(\n",
        "            self.vocab_size, self.emb_size, mask_zero=False, name='embedding'\n",
        "        )\n",
        "        out = embedding(state_in)\n",
        "        self.layers.append(embedding)\n",
        "\n",
        "        lstm = LSTM(\n",
        "            self.hidden_size, return_state=True, name='LSTM'\n",
        "        )\n",
        "        out, next_h, next_c = lstm(out, initial_state=[h_in, c_in])\n",
        "        self.layers.append(lstm)\n",
        "\n",
        "        dense = Dense(\n",
        "            self.vocab_size, activation='softmax', name='densesoftmax'\n",
        "        )\n",
        "        prob = dense(out)\n",
        "        self.layers.append(dense)\n",
        "\n",
        "        log_prob = tf.log(tf.reduce_sum(prob * action, axis=-1))\n",
        "        loss = - log_prob * reward\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
        "        minimize = optimizer.minimize(loss)\n",
        "\n",
        "        self.state_in = state_in\n",
        "        self.h_in = h_in\n",
        "        self.c_in = c_in\n",
        "        self.action = action\n",
        "        self.reward = reward\n",
        "        self.prob = prob\n",
        "        self.next_h = next_h\n",
        "        self.next_c = next_c\n",
        "        self.minimize = minimize\n",
        "        self.loss = loss\n",
        "\n",
        "        self.init_op = tf.global_variables_initializer()\n",
        "        self.sess.run(self.init_op)\n",
        "\n",
        "    def reset_rnn_state(self):\n",
        "        self.h = np.zeros([self.size, self.hidden_size])\n",
        "        self.c = np.zeros([self.size, self.hidden_size])\n",
        "\n",
        "    def set_rnn_state(self, h, c):\n",
        "        self.h = h\n",
        "        self.c = c\n",
        "\n",
        "    def get_rnn_state(self):\n",
        "        return self.h, self.c\n",
        "\n",
        "    def predict(self, state, stateful=True):\n",
        "        h = self.h\n",
        "        c = self.c\n",
        "        feed_dict = {\n",
        "            self.state_in: state,\n",
        "            self.h_in: h,\n",
        "            self.c_in: c\n",
        "        }\n",
        "        prob, next_h, next_c = self.sess.run(\n",
        "            [self.prob, self.next_h, self.next_c],\n",
        "            feed_dict\n",
        "        )\n",
        "\n",
        "        self.h = next_h\n",
        "        self.c = next_c\n",
        "        return prob, next_h, next_c\n",
        "\n",
        "    def update(self, state, action, reward, h=None, c=None, stateful=True):\n",
        "        if h is None:\n",
        "            h = self.h\n",
        "        if c is None:\n",
        "            c = self.c\n",
        "        state = state.reshape(-1, 1)\n",
        "        action = action.reshape(-1, 1)\n",
        "        reward = reward.reshape(-1, 1)\n",
        "        feed_dict = {\n",
        "            self.state_in: state,\n",
        "            self.h_in: h,\n",
        "            self.c_in: c,\n",
        "            self.action: to_categorical(action, self.vocab_size),\n",
        "            self.reward: reward\n",
        "        }\n",
        "        _, loss, next_h, next_c = self.sess.run(\n",
        "            [self.minimize, self.loss, self.next_h, self.next_c],\n",
        "            feed_dict\n",
        "        )\n",
        "\n",
        "        if stateful:\n",
        "            self.h = next_h\n",
        "            self.c = next_c\n",
        "            return loss\n",
        "        else:\n",
        "            return loss, next_h, next_c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJiybGTjBpyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import linecache\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "class Vocab(object):\n",
        "    def __init__(self, sentences_path):\n",
        "        self.PAD = 0\n",
        "        self.BOS = 1\n",
        "        self.EOS = 2\n",
        "        self.UNK = 3\n",
        "        self.PAD_TOKEN = '<PAD>'\n",
        "        self.BOS_TOKEN = '<S>'\n",
        "        self.EOS_TOKEN = '</S>'\n",
        "        self.UNK_TOKEN = '<UNK>'\n",
        "        self.word2id = {\n",
        "            self.PAD_TOKEN: self.PAD,\n",
        "            self.BOS_TOKEN: self.BOS,\n",
        "            self.EOS_TOKEN: self.EOS,\n",
        "            self.UNK_TOKEN: self.UNK,\n",
        "        }\n",
        "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
        "        self.sentences = load_data(sentences_path)\n",
        "        self.build_vocab(self.sentences)\n",
        "        self.vocab_num = len(self.word2id)\n",
        "        self.sentence_num = len(self.sentences)\n",
        "\n",
        "    def build_vocab(self, sentences, min_count=1):\n",
        "        word_counter = {}\n",
        "        for sentence in sentences:\n",
        "            for word in sentence:\n",
        "                word_counter[word] = word_counter.get(word, 0) + 1\n",
        "        for word, count in sorted(\n",
        "            word_counter.items(), key=lambda x: x[1], reverse=True\n",
        "        ):\n",
        "            if count < min_count:\n",
        "                break\n",
        "            _id = len(self.word2id)\n",
        "            self.word2id.setdefault(word, _id)\n",
        "            self.id2word[_id] = word\n",
        "\n",
        "    def write_word2id(self, sentences_path, output_path):\n",
        "        ids_sentences = []\n",
        "        for line in open(sentences_path, encoding='utf-8',errors='ignore'):\n",
        "            words = line.strip().split()\n",
        "            ids_words = [\n",
        "                str(self.word2id.get(word, self.UNK)) for word in words\n",
        "            ]\n",
        "            ids_sentences.append(ids_words)\n",
        "        self.data_num = len(ids_sentences)\n",
        "        output_str = ''\n",
        "        for i in range(count_data(sentences_path)):\n",
        "            output_str += ' '.join(ids_sentences[i]) + '\\n'\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(output_str)\n",
        "\n",
        "    def write_id2word(self, ids_path, output_path):\n",
        "        sentences = []\n",
        "        for line in open(ids_path, encoding='utf-8',errors='ignore'):\n",
        "            ids = line.strip().split()\n",
        "            words_ids = [\n",
        "                self.id2word.get(int(id), self.UNK_TOKEN) for id in ids\n",
        "            ]\n",
        "            sentences.append(words_ids)\n",
        "        output_str = ''\n",
        "        for i in range(count_data(ids_path)):\n",
        "            output_str += ' '.join(sentences[i]) + '\\n'\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(output_str)\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = []\n",
        "    for line in open(file_path, encoding='utf-8',errors='ignore'):\n",
        "        words = line.strip().split()\n",
        "        data.append(words)\n",
        "    return data\n",
        "\n",
        "\n",
        "def count_data(file_path):\n",
        "    data_num = 0\n",
        "    for line in open(file_path, encoding='utf-8',errors='ignore'):\n",
        "        data_num += 1\n",
        "    return data_num\n",
        "\n",
        "\n",
        "def padding(sentences, T, PAD=0):\n",
        "    sentences += [PAD for i in range(T - len(sentences))]\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def sentence_to_ids(vocab, sentence, UNK=3):\n",
        "    ids = [vocab.word2id.get(word, UNK) for word in sentence]\n",
        "    return ids\n",
        "\n",
        "\n",
        "class DataForGenerator(Sequence):\n",
        "    def __init__(self, ids_path, batch_size, T, vocab, shuffle=True):\n",
        "        self.ids_path = ids_path\n",
        "        self.batch_size = batch_size\n",
        "        self.n_data = count_data(ids_path)\n",
        "        self.T = T\n",
        "        self.shuffle = shuffle\n",
        "        self.vocab = vocab\n",
        "        self.reset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_data // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        batch_start = idx * self.batch_size\n",
        "        batch_end = (idx + 1) * self.batch_size\n",
        "        for i in range(batch_start, batch_end):\n",
        "            each_x = []\n",
        "            each_y = []\n",
        "            line_index = self.shuffled_indices[i] + 1\n",
        "            id_sentence = linecache.getline(self.ids_path, line_index)\n",
        "            id_words = id_sentence.strip().split()\n",
        "\n",
        "            each_x = [self.vocab.BOS, *id_words, self.vocab.EOS]\n",
        "            batch_x.append(each_x)\n",
        "\n",
        "            each_y = [*id_words, self.vocab.EOS]\n",
        "            batch_y.append(each_y)\n",
        "\n",
        "        for i, id_words in enumerate(batch_x):\n",
        "            batch_x[i] = batch_x[i][:self.T]\n",
        "        for i, id_words in enumerate(batch_y):\n",
        "            batch_y[i] = batch_y[i][:self.T]\n",
        "\n",
        "        batch_x = [padding(sentences, self.T) for sentences in batch_x]\n",
        "        batch_x = np.array(batch_x, dtype=np.int32)\n",
        "\n",
        "        batch_y = [padding(sentences, self.T) for sentences in batch_y]\n",
        "        batch_y = np.array(batch_y, dtype=np.int32)\n",
        "        batch_y = to_categorical(batch_y, num_classes=self.vocab.vocab_num)\n",
        "\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def reset(self):\n",
        "        self.idx = 0\n",
        "        if self.shuffle:\n",
        "            self.shuffled_indices = np.arange(self.n_data)\n",
        "            random.shuffle(self.shuffled_indices)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "class DataForDiscriminator(Sequence):\n",
        "    def __init__(self, pos_path, neg_path, batch_size, T, vocab, shuffle=True):\n",
        "        self.pos_path = pos_path\n",
        "        self.neg_path = neg_path\n",
        "        self.batch_size = batch_size\n",
        "        self.T = T\n",
        "        self.shuffle = shuffle\n",
        "        self.vocab = vocab\n",
        "        self.pos_n_data = count_data(pos_path)\n",
        "        self.neg_n_data = count_data(neg_path)\n",
        "        self.n_data = self.pos_n_data + self.neg_n_data\n",
        "        self.reset()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_data // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = []\n",
        "        batch_y = []\n",
        "        batch_start = idx * self.batch_size\n",
        "        batch_end = (idx + 1) * self.batch_size\n",
        "        for i in range(batch_start, batch_end):\n",
        "            each_x = []\n",
        "            line_index = self.shuffled_indices[i] + 1\n",
        "            if line_index < self.pos_n_data:\n",
        "                id_sentence = linecache.getline(self.pos_path, line_index)\n",
        "                is_pos = 1\n",
        "            else:\n",
        "                line_index = line_index - self.pos_n_data\n",
        "                id_sentence = linecache.getline(self.neg_path, line_index)\n",
        "                is_pos = 0\n",
        "            id_words = id_sentence.strip().split()\n",
        "\n",
        "            each_x = [*id_words, self.vocab.EOS]\n",
        "            batch_x.append(each_x)\n",
        "            batch_y.append(is_pos)\n",
        "            linecache.clearcache()\n",
        "\n",
        "        for i, id_words in enumerate(batch_x):\n",
        "            batch_x[i] = batch_x[i][:self.T]\n",
        "\n",
        "        batch_x = [padding(sentences, self.T) for sentences in batch_x]\n",
        "        batch_x = np.array(batch_x, dtype=np.int32)\n",
        "\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def reset(self):\n",
        "        self.idx = 0\n",
        "        if self.shuffle:\n",
        "            self.shuffled_indices = np.arange(self.n_data)\n",
        "            random.shuffle(self.shuffled_indices)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RilSkGBzByVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense, Embedding, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "class Environment(object):\n",
        "    def __init__(self, batch_size, vocab_size, emb_size, hidden_size,\n",
        "                 T, dropout, lr):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_size = emb_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.T = T\n",
        "        self.batch_size = batch_size\n",
        "        self.dropout = dropout\n",
        "        self.lr = lr\n",
        "        self.discriminator = self._build_graph(\n",
        "            self.vocab_size,\n",
        "            self.emb_size,\n",
        "            self.hidden_size,\n",
        "            self.dropout\n",
        "        )\n",
        "\n",
        "    def _build_graph(self, vocab_size, emb_size, hidden_size, dropout):\n",
        "        data_inp = Input(shape=(None, ), dtype='int32', name='input')\n",
        "        out = Embedding(\n",
        "            vocab_size, emb_size, mask_zero=False, name='embedding'\n",
        "        )(data_inp)\n",
        "        out = LSTM(hidden_size)(out)\n",
        "        out = Dropout(dropout, name='dropout')(out)\n",
        "        out = Dense(1, activation='sigmoid', name='dense_sigmoid')(out)\n",
        "        discriminator = Model(data_inp, out)\n",
        "        return discriminator\n",
        "\n",
        "    def pre_train(self, d_data, d_pre_episodes, d_pre_weight, d_pre_lr):\n",
        "        d_optimizer = Adam(d_pre_lr)\n",
        "        self.discriminator.compile(d_optimizer, 'binary_crossentropy')\n",
        "        self.discriminator.summary()\n",
        "        self.discriminator.fit_generator(\n",
        "            d_data,\n",
        "            steps_per_epoch=None,\n",
        "            epochs=d_pre_episodes\n",
        "        )\n",
        "        self.discriminator.save_weights(d_pre_weight)\n",
        "\n",
        "    def initialize(self, d_pre_weight):\n",
        "        self.discriminator.load_weights(d_pre_weight)\n",
        "        d_optimizer = Adam(self.lr)\n",
        "        self.discriminator.compile(d_optimizer, 'binary_crossentropy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPsmmI-CB2vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "overview:\n",
        "SeqGANの学習を行う\n",
        "\n",
        "args:\n",
        "各種パラメータ設定値は、本コード中に明記される\n",
        "\n",
        "output:\n",
        "フォルダ data に以下の要素が出力される\n",
        "- pre_generated_sentences.txt: 事前学習した生成器が生成した文章データ\n",
        "- pre_id_generated_sentences.txt: 事前学習した生成器が生成した文章idデータ\n",
        "- generated_sentences.txt: 強化学習した生成器が生成した文章データ\n",
        "- id_generated_sentences.txt: 強化学習した生成器が生成した文章idデータ\n",
        "フォルダ save に以下の要素が出力される\n",
        "- pre_d_weights.h5: 事前学習した識別器の重みパラメータ\n",
        "- pre_g_weights.h5: 事前学習した生成器の重みパラメータ\n",
        "- adversarial_n_generated_sentences.txt: nエピソード時の生成器が生成した文章データ\n",
        "- adversarial_n_id_generated_sentences.txt: nエピソード時の生成器が生成した文章idデータ\n",
        "\n",
        "usage-example:\n",
        "python3 main.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "#モジュールのインポートは省略\n",
        "\n",
        "sess = tf.Session()\n",
        "K.set_session(sess)\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 30\n",
        "T = 25  # max_length of sentences\n",
        "emb_size = 128  # embedding size\n",
        "g_hidden = 128  # generator hidden size\n",
        "d_hidden = 64  # discriminator hidden size\n",
        "g_lr = 1e-3  # generator learning rate in the reinforcement learning\n",
        "d_lr = 1e-3  # discriminator learning rate in the reinforcement learning\n",
        "dropout = 0.0\n",
        "\n",
        "# pretraining parameters\n",
        "g_pre_lr = 1e-2  # generator pre_training learning rate\n",
        "d_pre_lr = 1e-2  # discriminator pre_training learning rate\n",
        "g_pre_episodes = 10  # generator pre_training epochs\n",
        "d_pre_episodes = 4  # discriminator pre_training epochs\n",
        "d_epochs = 1\n",
        "\n",
        "# training parameters\n",
        "adversarial_nums = 10\n",
        "g_train_nums = 1  # number of generator train per adversarial learning\n",
        "d_train_nums = 1  # number of discriminator train per adversarial learning\n",
        "g_episodes = 50  # sentence num per generator update\n",
        "n_sampling = 16  # number of monte carlo tree\n",
        "frequency = 1\n",
        "\n",
        "# preprocess\n",
        "input_data = os.path.join('data', '/content/yonezu_wakati.txt')\n",
        "id_input_data = os.path.join('data', '/content/yonezu_id.txt')\n",
        "\n",
        "#generatorへのデータを追加\n",
        "input_data2 = os.path.join('data','/content/kashi_wakati.txt')\n",
        "id_input_data2 = os.path.join('data','/content/kashi_id.txt')\n",
        "\n",
        "#元コードに戻る\n",
        "pre_output_data = os.path.join(\n",
        "    'data', 'pre_generated_sentences.txt')\n",
        "pre_id_output_data = os.path.join(\n",
        "    'data', 'pre_id_generated_sentences.txt')\n",
        "output_data = os.path.join('data',\n",
        "                           'generated_sentences.txt')\n",
        "id_output_data = os.path.join(\n",
        "    'data', 'id_generated_sentences.txt')\n",
        "os.makedirs('data/save', exist_ok=True)\n",
        "g_pre_weight = os.path.join('data', 'save',\n",
        "                            'pre_g_weights.h5')\n",
        "d_pre_weight = os.path.join('data', 'save',\n",
        "                            'pre_d_weights.h5')\n",
        "\n",
        "vocab = Vocab(input_data)\n",
        "vocab_size = vocab.vocab_num\n",
        "pos_sentence_num = vocab.sentence_num\n",
        "vocab.write_word2id(input_data, id_input_data)\n",
        "sampling_num = vocab.data_num\n",
        "\n",
        "#追加データのベクトル化\n",
        "vocab2 = Vocab(input_data2)\n",
        "vocab_size2 = vocab2.vocab_num\n",
        "pos_sentence_num2 = vocab2.sentence_num\n",
        "vocab2.write_word2id(input_data2, id_input_data2)\n",
        "sampling_num2 = vocab2.data_num\n",
        "\n",
        "#元コードに戻る\n",
        "env = Environment(batch_size, vocab_size, emb_size,\n",
        "                  d_hidden, T, dropout, d_lr)\n",
        "agent = Agent(sess, vocab_size2, emb_size, g_hidden, T,\n",
        "              g_lr)\n",
        "\n",
        "\n",
        "def pre_train():\n",
        "    g_data = DataForGenerator(\n",
        "        id_input_data2,    #id_input_data -> id_input_data2\n",
        "        batch_size,\n",
        "        T,\n",
        "        vocab2     #vocab ->vocab2\n",
        "    )\n",
        "    agent.pre_train(\n",
        "        g_data,\n",
        "        g_pre_episodes,\n",
        "        g_pre_weight,\n",
        "        g_pre_lr\n",
        "    )\n",
        "    agent.generate_id_samples(\n",
        "        agent.generator,\n",
        "        T,\n",
        "        sampling_num2,        #sumpling_num -> sumpling_num2\n",
        "        pre_id_output_data\n",
        "    )\n",
        "    vocab2.write_id2word(pre_id_output_data,\n",
        "                        pre_output_data)\n",
        "    d_data = DataForDiscriminator(\n",
        "        id_input_data,\n",
        "        pre_id_output_data,\n",
        "        batch_size,\n",
        "        T,\n",
        "        vocab\n",
        "    )\n",
        "    env.pre_train(d_data, d_pre_episodes, d_pre_weight,\n",
        "                  d_pre_lr)\n",
        "\n",
        "\n",
        "def train():\n",
        "    agent.initialize(g_pre_weight)\n",
        "    env.initialize(d_pre_weight)\n",
        "    for adversarial_num in range(adversarial_nums):\n",
        "\n",
        "        print('---------------------------------------------')\n",
        "        print('Adversarial Training: ', adversarial_num + 1)\n",
        "\n",
        "        for _ in range(g_train_nums):\n",
        "            g_train()\n",
        "\n",
        "        print('Generator is trained')\n",
        "\n",
        "        for _ in range(d_train_nums):\n",
        "            d_train()\n",
        "\n",
        "        print('Discriminator is trained')\n",
        "\n",
        "        if adversarial_num % frequency == 0:\n",
        "            sentences_history(\n",
        "                adversarial_num,\n",
        "                agent,\n",
        "                T,\n",
        "                vocab2,               #vocab -> vocab2\n",
        "                sampling_num2         #sampling_num -> sumpling_num2\n",
        "            )\n",
        "\n",
        "\n",
        "def g_train():\n",
        "    batch_states = np.array([[]], dtype=np.int32)\n",
        "    batch_actions = np.array([[]], dtype=np.int32)\n",
        "    batch_rewards = np.array([[]], dtype=np.float32)\n",
        "    batch_hs = np.array([[]], dtype=np.float32)\n",
        "    batch_cs = np.array([[]], dtype=np.float32)\n",
        "    for g_episode in range(g_episodes):\n",
        "        agent.reset_rnn_states()\n",
        "        states = np.zeros([1, 1], dtype=np.int32)\n",
        "        states[:, 0] = vocab2.BOS          #vocab -> vocab2\n",
        "        actions = np.array([[]], dtype=np.int32)\n",
        "        rewards = np.array([[]], dtype=np.float32)\n",
        "        hs = np.zeros([1, g_hidden], dtype=np.float32)\n",
        "        cs = np.zeros([1, g_hidden], dtype=np.float32)\n",
        "        for step in range(T):\n",
        "            action, next_h, next_c = agent.get_action(\n",
        "                states)\n",
        "            agent.rollouter.reset_rnn_state()\n",
        "            reward = mc_search(step, states, action,\n",
        "                               next_h, next_c)\n",
        "            states = np.concatenate([states, action],\n",
        "                                    axis=-1)\n",
        "            rewards = np.concatenate([rewards, reward],\n",
        "                                     axis=-1)\n",
        "            actions = np.concatenate([actions, action],\n",
        "                                     axis=-1)\n",
        "            hs = np.concatenate([hs, next_h], axis=0)\n",
        "            cs = np.concatenate([cs, next_c], axis=0)\n",
        "        states = states[:, :-1]\n",
        "        hs = hs[:-1]\n",
        "        cs = cs[:-1]\n",
        "        batch_states = np.concatenate(\n",
        "            [batch_states, states], axis=-1)\n",
        "        batch_actions = np.concatenate(\n",
        "            [batch_actions, actions], axis=-1)\n",
        "        batch_rewards = np.concatenate(\n",
        "            [batch_rewards, rewards], axis=-1)\n",
        "        batch_hs = np.append(batch_hs,\n",
        "                             hs).reshape(-1, g_hidden)\n",
        "        batch_cs = np.append(batch_cs,\n",
        "                             cs).reshape(-1, g_hidden)\n",
        "    agent.generator.update(batch_states, batch_actions,\n",
        "                           batch_rewards, batch_hs,\n",
        "                           batch_cs)\n",
        "    agent.inherit_weights(agent.generator,\n",
        "                          agent.rollouter)\n",
        "\n",
        "\n",
        "def d_train():\n",
        "    agent.generate_id_samples(\n",
        "        agent.generator,\n",
        "        T,\n",
        "        sampling_num2,                                    #sumpling_num -> sumpling_num2\n",
        "        id_output_data,\n",
        "    )\n",
        "    vocab2.write_id2word(id_output_data, output_data)     #vocab -> vocab2\n",
        "\n",
        "    d_data = DataForDiscriminator(id_input_data,\n",
        "                                  id_output_data,\n",
        "                                  batch_size, T, vocab)\n",
        "\n",
        "    env.discriminator.fit_generator(d_data,\n",
        "                                    steps_per_epoch=None,\n",
        "                                    epochs=1)\n",
        "\n",
        "\n",
        "def sentences_history(episode, agent, T, vocab,\n",
        "                      sampling_num):\n",
        "    id_output_history = os.path.join(\n",
        "        'data',\n",
        "        'adversarial_{}_id_generated_sentences.txt'.\n",
        "        format(episode + 1))\n",
        "    output_history = os.path.join(\n",
        "        'data',\n",
        "        'adversarial_{}_generated_sentences.txt'.format(\n",
        "            episode + 1))\n",
        "    agent.generate_id_samples(agent.generator, T,\n",
        "                              sampling_num,      \n",
        "                              id_output_history)\n",
        "    vocab.write_id2word(id_output_history,\n",
        "                        output_history)        \n",
        "\n",
        "\n",
        "def mc_search(step, states, action, next_h, next_c):\n",
        "    reward_t = np.zeros([1, 1], dtype=np.float32)\n",
        "    agent.rollouter.reset_rnn_state()\n",
        "    if step < T - 1:\n",
        "        agent.rollouter.set_rnn_state(next_h, next_c)\n",
        "        for i in range(n_sampling):\n",
        "            Y = agent.rollout(step, states, action)\n",
        "            reward_t += env.discriminator.predict(\n",
        "                Y) / n_sampling\n",
        "    else:\n",
        "        Y = np.concatenate([states[:, 1:], action],\n",
        "                           axis=-1)\n",
        "        reward_t = env.discriminator.predict(Y)\n",
        "    return reward_t\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pre_train()\n",
        "    train()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivJO7QGByw4j",
        "colab_type": "text"
      },
      "source": [
        "パーセプトロンによる生成データの精度確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8okK6MgYya8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#coding=UTF-8\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "#dfの準備\n",
        "lst = []\n",
        "\n",
        "with open('Desktop/Python3/AKB.txt','r') as f:\n",
        "    a = f.readlines()\n",
        "    lst = lst + a\n",
        "\n",
        "with open('Desktop/Python3/arashi.txt','r') as f:\n",
        "    a = f.readlines()\n",
        "    lst = lst + a\n",
        "\n",
        "with open('Desktop/Python3/BUMP.txt','r') as f:\n",
        "    a = f.readlines()\n",
        "    lst = lst + a\n",
        "\n",
        "\n",
        "with open('Desktop/Python3/ikimonogakari.txt','r') as f:\n",
        "    a = f.readlines()\n",
        "    lst = lst + a\n",
        "\n",
        "with open('Desktop/Python3/nakajimamiyuki.txt','r') as f:\n",
        "    a = f.readlines()\n",
        "    lst = lst + a\n",
        "\n",
        "with open('Desktop/Python3/nishinokana.txt','r') as f:\n",
        "    a = f.readlines()\n",
        "    lst = lst + a\n",
        "\n",
        "with open('Desktop/Python3/poruno.txt','r') as f:\n",
        "    a = f.readlines()\n",
        "    lst = lst + a\n",
        "\n",
        "with open('Desktop/Python3/shiinaringo.txt','r') as f:\n",
        "    a = f.readlines()\n",
        "    lst = lst + a\n",
        "\n",
        "with open('Desktop/Python3/SMAP.txt','r') as f:\n",
        "    a = f.readlines()\n",
        "    lst = lst + a\n",
        "\n",
        "\n",
        "with open('Desktop/Python3/yonedu_kashi.txt','r') as f:\n",
        "    a = f.readlines()\n",
        "    lst = lst + a\n",
        "\n",
        "\n",
        "lst_artist = ['AKB48'] * 200 + ['嵐']*200 + ['BUMP OF CHICKEN']*126  +['いきものがかり']*143+['中島みゆき']*200+['西野カナ']*172+['ポルノグラフティ']*198+['椎名林檎']*108+['SMAP']*196+['米津玄師']*80\n",
        "\n",
        "df = pd.DataFrame([lst,lst_artist],index=['lyrics','artist'])\n",
        "df = df.T\n",
        "\n",
        "\n",
        "import argparse\n",
        "import csv\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "import MeCab\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "\n",
        "#データセットの前処理\n",
        "def tokenize(tagger, text):\n",
        "    result = tagger.parse(text).strip()\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dir')\n",
        "parser.add_argument('--out', default='data')\n",
        "args = parser.parse_args()\n",
        "\n",
        "tagger = MeCab.Tagger('-Owakati')\n",
        "df['lyrics'] = df['lyrics'].map(lambda x:tagger.parse(str(x)))\n",
        "print(df.head(10))\n",
        "\n",
        "# 学習データとテストデータに分割\n",
        "train, test = train_test_split(df)\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    input='content')\n",
        "# 素性ベクトル変換器を構築し、学習データに適用\n",
        "train_titles = train['lyrics']\n",
        "train_X = vectorizer.fit_transform(train_titles)\n",
        "\n",
        "# テストデータに素性ベクトル変換器を適用\n",
        "test_titles = test['lyrics']\n",
        "test_X = vectorizer.transform(test_titles)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "# ラベルをIDへ変換する変換器を構築し、学習データに適用\n",
        "train_categories = train['artist']\n",
        "train_y = label_encoder.fit_transform(train_categories)\n",
        "\n",
        "# 構築したラベル変換器をテストデータに適用\n",
        "test_categories = test['artist']\n",
        "test_y = label_encoder.transform(test_categories)\n",
        "\n",
        "if not os.path.exists(args.out):\n",
        "    os.makedirs(args.out)\n",
        "\n",
        "train_file = os.path.join(args.out, 'train.pickle')\n",
        "test_file = os.path.join(args.out, 'test.pickle')\n",
        "vectorizer_file = os.path.join(args.out, 'vectorizer.pickle')\n",
        "label_encoder_file = os.path.join(args.out, 'label_encoder.pickle')\n",
        "\n",
        "with open(train_file, 'wb') as f:\n",
        "    pickle.dump([train_X, train_y], f)\n",
        "\n",
        "with open(test_file, 'wb') as f:\n",
        "    pickle.dump([test_X, test_y], f)\n",
        "\n",
        "with open(vectorizer_file, 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "with open(label_encoder_file, 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "\n",
        "\n",
        "#分類器の学習\n",
        "import argparse\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dir', default='data')\n",
        "args = parser.parse_args()\n",
        "\n",
        "train_file = os.path.join(args.dir, 'train.pickle')\n",
        "with open(train_file, 'rb') as f:\n",
        "    train_X, train_y = pickle.load(f)\n",
        "    print('train', train_X.shape, train_y.shape)\n",
        "\n",
        "test_file = os.path.join(args.dir, 'test.pickle')\n",
        "with open(test_file, 'rb') as f:\n",
        "    test_X, test_y = pickle.load(f)\n",
        "    print('test:', test_X.shape, test_y.shape)\n",
        "\n",
        "model = Perceptron(\n",
        "    penalty='l2',\n",
        "    shuffle=True,\n",
        "    verbose=2)\n",
        "\n",
        "# パーセプトロンを学習\n",
        "model.fit(train_X, train_y)\n",
        "# テストデータに対して予測\n",
        "test_y_pred = model.predict(test_X)\n",
        "\n",
        "# テストデータにおける適合率、再現率、F値を算出\n",
        "precision, recall, fscore, _ = precision_recall_fscore_support(\n",
        "    test_y,\n",
        "    test_y_pred,\n",
        "    average='micro')\n",
        "\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F-score:', fscore)\n",
        "\n",
        "model_file = os.path.join(args.dir, 'model.pickle')\n",
        "with open(model_file, 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import MeCab\n",
        "\n",
        "  # この記事の前処理用プログラム\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dir', default='data')\n",
        "args = parser.parse_args()\n",
        "\n",
        "model_file = os.path.join(args.dir, 'model.pickle')\n",
        "with open(model_file, 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "label_encoder_file = os.path.join(args.dir, 'label_encoder.pickle')\n",
        "with open(label_encoder_file, 'rb') as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "vectorizer_file = os.path.join(args.dir, 'vectorizer.pickle')\n",
        "with open(vectorizer_file, 'rb') as f:\n",
        "    vectorizer = pickle.load(f)\n",
        "\n",
        "tagger = MeCab.Tagger('-Owakati')\n",
        "\n",
        "\n",
        "text = 'ここにテキストを入力'\n",
        "tokenized = tokenize(tagger, text)\n",
        "\n",
        "x = vectorizer.transform([tokenized])\n",
        "y = model.predict(x)\n",
        "    # ラベルをIDから対応する文字に変換\n",
        "label = label_encoder.inverse_transform(y)[0]\n",
        "print('Tokenized:', tokenized)\n",
        "print('Label:', label)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}